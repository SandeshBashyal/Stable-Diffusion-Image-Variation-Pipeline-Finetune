{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rmuproject/rmuproject/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionImageVariationPipeline\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import torchvision\n",
    "\n",
    "from load_data import CustomImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'device': 'cuda'} are not expected by StableDiffusionImageVariationPipeline and will be ignored.\n",
      "Loading pipeline components...: 100%|██████████| 5/5 [00:00<00:00, 15.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the pipeline and components\n",
    "pipeline = StableDiffusionImageVariationPipeline.from_pretrained(\"/home/rmuproject/rmuproject/users/sandesh/models/80_epochs/\",\n",
    "                                                                 requires_safety_checker=False,\n",
    "                                                                 device='cuda')\n",
    "pipeline.enable_model_cpu_offload()\n",
    "unet = pipeline.unet\n",
    "vae = pipeline.vae\n",
    "clip_encoder = pipeline.image_encoder.to('cpu')\n",
    "# feature_extractor = pipeline.feature_extractor.to('cpu')\n",
    "# Freeze the VAE and CLIP encoder\n",
    "for param in vae.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in clip_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_name = \"/home/rmuproject/rmuproject/data\"  # @param\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, dataset, clip_encoder, feature_extractor, size=512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: A dataset object from the `datasets` library.\n",
    "            clip_encoder: The CLIP image encoder model (e.g., CLIPVisionModelWithProjection).\n",
    "            feature_extractor: The feature extractor from the StableDiffusionImageVariationPipeline.\n",
    "            size: The size to which images should be resized (default: 512x512).\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.clip_encoder = clip_encoder\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.size = size\n",
    "\n",
    "        # Transformations for the input images (resize, normalize, etc.)\n",
    "        self.transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((size, size)),  # Resize to the required size\n",
    "                transforms.CenterCrop(size),      # Center crop to ensure square images\n",
    "                transforms.ToTensor(),            # Convert to tensor\n",
    "                transforms.Normalize([0.5], [0.5]),  # Normalize to [-1, 1]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        try:\n",
    "            # Load the image\n",
    "            image = self.dataset[index][\"image\"]\n",
    "            if not isinstance(image, Image.Image):\n",
    "                image = Image.open(image).convert(\"RGB\")  # Ensure it's a PIL image\n",
    "\n",
    "            if image.mode != \"RGB\":\n",
    "                image = image.convert(\"RGB\")\n",
    "\n",
    "            # Transform the image for the UNet\n",
    "            example[\"instance_images\"] = self.transforms(image)\n",
    "\n",
    "            # Generate the CLIP embedding\n",
    "            with torch.no_grad():\n",
    "                # Preprocess the image for CLIP using the feature_extractor\n",
    "                clip_input = self.feature_extractor(image, return_tensors=\"pt\").pixel_values.squeeze(0)  # Shape: [3, 224, 224]\n",
    "                clip_input = clip_input.to(self.clip_encoder.device)  # Move to the correct device\n",
    "                clip_embedding = self.clip_encoder(clip_input.unsqueeze(0)).image_embeds  # Shape: [1, embedding_dim]\n",
    "\n",
    "            # Add a sequence length dimension to the CLIP embedding\n",
    "            clip_embedding = clip_embedding.unsqueeze(1).to('cpu')  # Shape: [1, 1, embedding_dim]\n",
    "            example[\"clip_embeddings\"] = clip_embedding.squeeze(0)  # Remove batch dimension for collation\n",
    "\n",
    "        except Exception as e:\n",
    "            # Skip corrupted or invalid images\n",
    "            print(f\"Error processing image at index {index}: {e}\")\n",
    "            return self.__getitem__((index + 1) % len(self))  # Skip to the next image\n",
    "\n",
    "        return example  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 512, 512])\n",
      "torch.Size([1, 768])\n",
      "dict_keys(['instance_images', 'clip_embeddings'])\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = pipeline.feature_extractor\n",
    "# Create the dataset\n",
    "image_variation_dataset = CustomImageDataset(dataset, clip_encoder, feature_extractor)\n",
    "\n",
    "# Example: Access a single item\n",
    "example = image_variation_dataset[0]\n",
    "print(example[\"instance_images\"].shape)  # Should be [3, 512, 512]\n",
    "print(example[\"clip_embeddings\"].shape)  # Should be [embedding_dim]\n",
    "example = image_variation_dataset[0]\n",
    "print(example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(examples):\n",
    "    \"\"\"\n",
    "    Collate function for the ImageVariationDataset.\n",
    "    Args:\n",
    "        examples: A list of dictionaries, where each dictionary contains:\n",
    "            - \"instance_images\": A tensor of shape [C, H, W].\n",
    "            - \"clip_embeddings\": A tensor of shape [embedding_dim].\n",
    "    Returns:\n",
    "        A dictionary containing:\n",
    "            - \"instance_images\": A stacked tensor of shape [batch_size, C, H, W].\n",
    "            - \"clip_embeddings\": A stacked tensor of shape [batch_size, embedding_dim].\n",
    "    \"\"\"\n",
    "    # Extract instance_images and clip_embeddings from the examples\n",
    "    instance_images = [example[\"instance_images\"] for example in examples]\n",
    "    clip_embeddings = [example[\"clip_embeddings\"] for example in examples]\n",
    "\n",
    "    # Stack the tensors along the batch dimension\n",
    "    instance_images = torch.stack(instance_images)\n",
    "    clip_embeddings = torch.stack(clip_embeddings)\n",
    "\n",
    "    # Ensure the instance_images tensor is in contiguous memory format and cast to float\n",
    "    instance_images = instance_images.to(memory_format=torch.contiguous_format).float()\n",
    "\n",
    "    # Return the batch as a dictionary\n",
    "    batch = {\n",
    "        \"instance_images\": instance_images,  # Shape: [batch_size, C, H, W]\n",
    "        \"clip_embeddings\": clip_embeddings,  # Shape: [batch_size, embedding_dim]\n",
    "    }\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512, 512])\n",
      "torch.Size([1, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 1\n",
    "# Assuming `dataset` is your ImageVariationDataset\n",
    "dataloader = DataLoader(image_variation_dataset, batch_size= batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "for batch in dataloader:\n",
    "    instance_images = batch[\"instance_images\"]  # Shape: [batch_size, C, H, W]\n",
    "    clip_embeddings = batch[\"clip_embeddings\"]  # Shape: [batch_size, embedding_dim]\n",
    "    # Pass these to your model or pipeline\n",
    "    print(instance_images.shape)\n",
    "    print(clip_embeddings.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2e-06\n",
    "max_train_steps = 400\n",
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(\n",
    "    # pretrained_model_name_or_path=model_id,\n",
    "    resolution=512,  # Reduce this if you want to save some memory\n",
    "    train_dataset=image_variation_dataset,\n",
    "    resume_from_checkpoint = None,\n",
    "     checkpointing_steps = 200,\n",
    "    # instance_prompt=instance_prompt,\n",
    "    learning_rate=learning_rate,\n",
    "    max_train_steps=max_train_steps,\n",
    "    train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,  # Increase this if you want to lower memory usage\n",
    "    max_grad_norm=1.0,\n",
    "    gradient_checkpointing=True,  # Set this to True to lower the memory usage\n",
    "    use_8bit_adam=True,  # Use 8bit optimizer from bitsandbytes\n",
    "    seed=3434554,\n",
    "    sample_batch_size=2,\n",
    "    output_dir=\"/home/rmuproject/rmuproject/users/sandesh/models/new\",  # Where to save the pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from diffusers import DDPMScheduler, PNDMScheduler, StableDiffusionImageVariationPipeline\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from lpips import LPIPS\n",
    "\n",
    "def training_function(vae, unet, clip_encoder):\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        mixed_precision=\"fp16\",  # Enable mixed precision\n",
    "    )\n",
    "\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    if args.gradient_checkpointing:\n",
    "        unet.enable_gradient_checkpointing()\n",
    "\n",
    "    # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\n",
    "    if args.use_8bit_adam:\n",
    "        import bitsandbytes as bnb\n",
    "\n",
    "        optimizer_class = bnb.optim.AdamW8bit\n",
    "    else:\n",
    "        optimizer_class = torch.optim.AdamW\n",
    "\n",
    "    optimizer = optimizer_class(\n",
    "        unet.parameters(),  # Only optimize unet\n",
    "        lr=args.learning_rate,\n",
    "    )\n",
    "\n",
    "    lr_scheduler = CosineAnnealingLR(optimizer, T_max=args.max_train_steps, eta_min=1e-7)\n",
    "    \n",
    "    noise_scheduler = DDPMScheduler(\n",
    "        beta_start=0.00085,\n",
    "        beta_end=0.012,\n",
    "        beta_schedule=\"scaled_linear\",\n",
    "        num_train_timesteps=1000,\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        args.train_dataset,\n",
    "        batch_size=args.train_batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    unet, optimizer, train_dataloader = accelerator.prepare(unet, optimizer, train_dataloader)\n",
    "\n",
    "    # Move vae and clip_encoder to the accelerator device\n",
    "    vae.to(accelerator.device)\n",
    "    clip_encoder.to(accelerator.device)\n",
    "\n",
    "    # We need to recalculate our total training steps as the size of the training dataloader may have changed\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "    num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "    # Train!\n",
    "    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "    progress_bar.set_description(\"Steps\")\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(num_train_epochs):\n",
    "        unet.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            with accelerator.accumulate(unet):\n",
    "                # Convert images to latent space\n",
    "                with torch.no_grad():\n",
    "                    latents = vae.encode(batch[\"instance_images\"].to(accelerator.device)).latent_dist.sample()\n",
    "                    latents = latents * 0.18215  # Scale the latents (VAE scaling factor)\n",
    "\n",
    "                # Sample noise that we'll add to the latents\n",
    "                noise = torch.randn(latents.shape).to(latents.device)\n",
    "                bsz = latents.shape[0]\n",
    "\n",
    "                # Sample a random timestep for each image\n",
    "                timesteps = torch.randint(\n",
    "                    0,\n",
    "                    noise_scheduler.config.num_train_timesteps,\n",
    "                    (bsz,),\n",
    "                    device=latents.device,\n",
    "                ).long()\n",
    "\n",
    "                # Add noise to the latents according to the noise magnitude at each timestep\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "                # Get the CLIP embeddings for conditioning\n",
    "                clip_embeddings = batch[\"clip_embeddings\"].to(accelerator.device)\n",
    "                # Predict the noise residual\n",
    "                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states=clip_embeddings).sample\n",
    "\n",
    "                # Calculate the loss\n",
    "                loss = F.mse_loss(noise_pred, noise, reduction=\"none\").mean([1, 2, 3]).mean()\n",
    "\n",
    "                # Backpropagate\n",
    "                accelerator.backward(loss)\n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(unet.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                lr_scheduler.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.update(1)\n",
    "                global_step += 1\n",
    "\n",
    "            logs = {\"loss\": loss.detach().item() * args.gradient_accumulation_steps}  # Rescale loss for logging\n",
    "            progress_bar.set_postfix(**logs)\n",
    "\n",
    "            if global_step >= args.max_train_steps:\n",
    "                break\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "\n",
    "    # Create the pipeline using the trained modules and save it\n",
    "    if accelerator.is_main_process:\n",
    "        print(f\"Loading pipeline and saving to {args.output_dir}...\")\n",
    "        scheduler = PNDMScheduler(\n",
    "            beta_start=0.00085,\n",
    "            beta_end=0.012,\n",
    "            beta_schedule=\"scaled_linear\",\n",
    "            skip_prk_steps=True,\n",
    "            steps_offset=1,\n",
    "        )\n",
    "        pipeline = StableDiffusionImageVariationPipeline(\n",
    "            vae=vae,\n",
    "            unet=accelerator.unwrap_model(unet),\n",
    "            scheduler=scheduler,\n",
    "            image_encoder=clip_encoder,\n",
    "            safety_checker= None,\n",
    "            feature_extractor=feature_extractor\n",
    "        )\n",
    "        pipeline.save_pretrained(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_function(vae, unet, clip_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
